---
tags:
  - reference/paper/journal
aliases:
  - "Representation Learning: A Review and New Perspectives"
  - "bengio2013representation"
---
# [Representation Learning: A Review and New Perspectives](http://ieeexplore.ieee.org/document/6472238/)
**Collections**:: [[Surveys]]
## Abstract
The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.
## Annotations
+ **Highlight**: "<mark style="background: Orange;">One reason why explicitly dealing with representations is interesting is because they can be convenient to express many general priors about the world around us, i.e., priors that are not task specific but would be likely to be useful for a learning machine to solve AI tasks.</mark>"<br>- On [page 3](zotero://open-pdf/library/items/5DX6KZ42?page=1800&annotation=NPWTINFJ) ^NPWTINFJ
+ **Highlight**: "<mark style="background: Green;">Smoothness: Assumes that the function to be learned f is s.t. x   y generally implies fðxÞ fðyÞ. This most basic prior is present in most machine learning, butisinsufficienttogetaroundthecurseof dimensionality; see Section 3.2.</mark>"<br>- On [page 3](zotero://open-pdf/library/items/5DX6KZ42?page=1800&annotation=FDLIII9N) ^FDLIII9N
+ **Highlight**: "<mark style="background: Green;">Multiple explanatory factors: The data generating distribution is generated by different underlying factors, and for the most part, what one learns about one factor generalizes in many configurations of the other factors. The objective to recover or at least disentangle these underlying factors of variation is discussed in Section 3.5. This assumption is behind the idea of distributed representations, discussed in Section 3.3.</mark>"<br>- On [page 3](zotero://open-pdf/library/items/5DX6KZ42?page=1800&annotation=LEMUP3RU) ^LEMUP3RU
+ **Highlight**: "<mark style="background: Green;">A hierarchical organization of explanatory factors: The concepts that are useful for describing the world around us can be defined in terms of other concepts, in a hierarchy, with more abstract concepts higher in the hierarchy, defined in terms of less abstract ones. This assumption is exploited with deep representations, elaborated in Section 3.4.</mark>"<br>- On [page 3](zotero://open-pdf/library/items/5DX6KZ42?page=1800&annotation=UUVQAMGK) ^UUVQAMGK
+ **Highlight**: "<mark style="background: Green;">Semi-supervised learning: With inputs X and target Y to predict, a subset of the factors explaining X’s distribution explain much of Y , given X. Hence, representations that are useful for P ðXÞ tend to be useful when learning P ðY j XÞ, allowing sharing of statistical strength between the unsupervised and supervised learning tasks, see Section 4.</mark>"<br>- On [page 3](zotero://open-pdf/library/items/5DX6KZ42?page=1800&annotation=XKNKPEA4) ^XKNKPEA4
+ **Highlight**: "<mark style="background: Green;">Shared factors across tasks: With many Y s of interest or many learning tasks in general, tasks (e.g., the corresponding P ðY j X; taskÞ)areexplainedby factors that are shared with other tasks, allowing sharing of statistical strengths across tasks, as</mark>"<br>- On [page 3](zotero://open-pdf/library/items/5DX6KZ42?page=1800&annotation=AAHBNKY9) ^AAHBNKY9
+ **Highlight**: "<mark style="background: Green;">+discussed in the previous section (multitask and transfer learning, domain adaptation).</mark>"<br>- On [page 3](zotero://open-pdf/library/items/5DX6KZ42?page=1800&annotation=NJTTCZ8N) ^NJTTCZ8N
+ **Highlight**: "<mark style="background: Green;">Manifolds: Probability mass concentrates near regions that have a much smaller dimensionality than the original space where the data live. This is explicitly exploited in some of the autoencoder algorithms and other manifold-inspired algorithms described, respectively, in Sections 7.2 and 8.</mark>"<br>- On [page 3](zotero://open-pdf/library/items/5DX6KZ42?page=1800&annotation=Y2FR4PL4) ^Y2FR4PL4
+ **Highlight**: "<mark style="background: Green;">Natural clustering: Different values of categorical variables such as object classes are associated with separate manifolds. More precisely, the local variations on the manifold tend to preserve the value of a category, and a linear interpolation between examples of different classes in general involves going through a low-density region, i.e., P ðX j Y ¼ iÞ for different i tend to be well separated and not overlap much. For example, this is exploited in the manifold tangent classifier (MTC) discussed in Section 8.3. This hypothesis is consistent with the idea that humans have named categories and classes because of such statistical structure (discovered by their brain and propagated by their culture), and machine learning tasks often involve predicting such categorical variables.</mark>"<br>- On [page 3](zotero://open-pdf/library/items/5DX6KZ42?page=1800&annotation=CUQI9Y22) ^CUQI9Y22
+ **Highlight**: "<mark style="background: Green;">Temporal and spatial coherence: Consecutive (from a sequence) or spatially nearby observations tend to be associated with the same value of relevant categorical concepts or result in a small move on the surface of the high-density manifold. More generally, different factors change at different temporal and spatial scales, and many categorical concepts of interest change slowly. When attempting to capture such categorical variables, this prior can be enforced by making the associated representations slowly changing, i.e., penalizing changes in values over time or space. This prior was introduced in [6] and is discussed in Section 11.3.</mark>"<br>- On [page 3](zotero://open-pdf/library/items/5DX6KZ42?page=1800&annotation=MDICG6M9) ^MDICG6M9
+ **Highlight**: "<mark style="background: Green;">Sparsity: For any given observation x, only a small fraction of the possible factors are relevant. In terms of representation, this could be represented by features that are often zero (as initially proposed by Olshausen and Field [155]), or by the fact that most of the extracted features are insensitive to small variations of x. This can be achieved with certain forms of priors on latent variables (peaked at 0), or by using a nonlinearity whose value is often flat at 0 (i.e., 0 and with a 0 derivative), or simply by penalizing the magnitude of the Jacobian matrix (of derivatives) of the function mapping input to representation. This is discussed in Sections 6.1.1 and 7.2.</mark>"<br>- On [page 3](zotero://open-pdf/library/items/5DX6KZ42?page=1800&annotation=KYTP8JQ9) ^KYTP8JQ9
+ **Highlight**: "<mark style="background: Green;">Simplicity of factor dependencies: In good high-level representations, the factors are related to each other through simple, typically linear dependencies. This can be seen in many laws of physics and is assumed when plugging a linear predictor on top of a learned representation.</mark>"<br>- On [page 3](zotero://open-pdf/library/items/5DX6KZ42?page=1800&annotation=BAYA2MYL) ^BAYA2MYL
## Metadata
+ **Topics**::  [[Representation Learning]]
+ **Authors**:: [[Bengio, Yoshua]], [[Courville, Aaron]], [[Vincent, Pascal]]
+ **Journal**:: [[IEEE Transactions on Pattern Analysis and Machine Intelligence#Volume 35 - Issue 8|IEEE Transactions on Pattern Analysis and Machine Intelligence]]
+ **Year**: [[2013]]
## Resources
+ [Full text](https://arxiv.org/pdf/1206.5538.pdf)
